{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/J1406/myproyet_/blob/main/Objects_DetectionJM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision requests validators transformers qrcode gradio -q"
      ],
      "metadata": {
        "id": "Jz5ZvWxqzqDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18709360-c1e9-4db7-e48d-f287f31a16c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m698.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -q"
      ],
      "metadata": {
        "id": "jhuhsznOvCH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import gradio as gr\n",
        "import matplotlib.pyplot as plt\n",
        "import requests, validators\n",
        "import torch\n",
        "import pathlib\n",
        "from PIL import Image\n",
        "from transformers import AutoFeatureExtractor, DetrForObjectDetection, YolosForObjectDetection\n",
        "\n",
        "import os\n",
        "\n",
        "# colors for visualization\n",
        "COLORS = [\n",
        "    [0.000, 0.447, 0.741],\n",
        "    [0.850, 0.325, 0.098],\n",
        "    [0.929, 0.694, 0.125],\n",
        "    [0.494, 0.184, 0.556],\n",
        "    [0.466, 0.674, 0.188],\n",
        "    [0.301, 0.745, 0.933]\n",
        "]\n",
        "\n",
        "def make_prediction(img, feature_extractor, model):\n",
        "    inputs = feature_extractor(img, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs)\n",
        "    img_size = torch.tensor([tuple(reversed(img.size))])\n",
        "    processed_outputs = feature_extractor.post_process(outputs, img_size)\n",
        "    return processed_outputs[0]\n",
        "\n",
        "def fig2img(fig):\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf)\n",
        "    buf.seek(0)\n",
        "    img = Image.open(buf)\n",
        "    return img\n",
        "\n",
        "\n",
        "def visualize_prediction(pil_img, output_dict, threshold=0.7, id2label=None):\n",
        "    keep = output_dict[\"scores\"] > threshold\n",
        "    boxes = output_dict[\"boxes\"][keep].tolist()\n",
        "    scores = output_dict[\"scores\"][keep].tolist()\n",
        "    labels = output_dict[\"labels\"][keep].tolist()\n",
        "    if id2label is not None:\n",
        "        labels = [id2label[x] for x in labels]\n",
        "\n",
        "    plt.figure(figsize=(16, 10))\n",
        "    plt.imshow(pil_img)\n",
        "    ax = plt.gca()\n",
        "    colors = COLORS * 100\n",
        "    for score, (xmin, ymin, xmax, ymax), label, color in zip(scores, boxes, labels, colors):\n",
        "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=color, linewidth=3))\n",
        "        ax.text(xmin, ymin, f\"{label}: {score:0.2f}\", fontsize=15, bbox=dict(facecolor=\"yellow\", alpha=0.5))\n",
        "    plt.axis(\"off\")\n",
        "    return fig2img(plt.gcf())\n",
        "\n",
        "def detect_objects(model_name,url_input,image_input,threshold):\n",
        "\n",
        "    #Extract model and feature extractor\n",
        "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
        "\n",
        "    if 'detr' in model_name:\n",
        "\n",
        "        model = DetrForObjectDetection.from_pretrained(model_name)\n",
        "\n",
        "    elif 'yolos' in model_name:\n",
        "\n",
        "        model = YolosForObjectDetection.from_pretrained(model_name)\n",
        "\n",
        "    if validators.url(url_input):\n",
        "        image = Image.open(requests.get(url_input, stream=True).raw)\n",
        "\n",
        "    elif image_input:\n",
        "        image = image_input\n",
        "\n",
        "    #Make prediction\n",
        "    processed_outputs = make_prediction(image, feature_extractor, model)\n",
        "\n",
        "    #Visualize prediction\n",
        "    viz_img = visualize_prediction(image, processed_outputs, threshold, model.config.id2label)\n",
        "\n",
        "    return viz_img\n",
        "\n",
        "def set_example_image(example: list) -> dict:\n",
        "    return gr.Image.update(value=example[0])\n",
        "\n",
        "def set_example_url(example: list) -> dict:\n",
        "    return gr.Textbox.update(value=example[0])\n",
        "\n",
        "\n",
        "title = \"\"\"<h1 id=\"title\">Aplicación de detección de objetos con YOLOS</h1>\"\"\"\n",
        "\n",
        "\n",
        "models = ['hustvl/yolos-small','hustvl/yolos-tiny']\n",
        "urls = [\"https://media.npr.org/assets/img/2023/08/14/defconai-8321_slide-648569f04a3571dc4733dca65d2c0ca3ec672f50.jpg\"]\n",
        "\n",
        "\n",
        "demo = gr.Blocks()\n",
        "\n",
        "with demo:\n",
        "\n",
        "    gr.Markdown(title)\n",
        "    options = gr.Dropdown(choices=models, label='Seleccione el modelo de detección qué desea usar', show_label=True)\n",
        "    slider_input = gr.Slider(minimum=0.2, maximum=1, value=0.7, label='Umbral de predicción')\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem('Imagen atraves de un URL'):\n",
        "            with gr.Row():\n",
        "                url_input = gr.Textbox(lines=2,label='Ingrese una URL valida')\n",
        "                img_output_from_url = gr.Image() #shape=(650,650)\n",
        "\n",
        "            with gr.Row():\n",
        "                example_url = gr.Dataset(components=[url_input],samples=[[str(url)] for url in urls])\n",
        "\n",
        "            url_but = gr.Button('Detectar')\n",
        "\n",
        "        with gr.TabItem('Subir imagen'):\n",
        "            with gr.Row():\n",
        "                img_input = gr.Image(type='pil')\n",
        "                img_output_from_upload= gr.Image() #shape=(650,650)\n",
        "\n",
        "            with gr.Row():\n",
        "                example_images = gr.Dataset(components=[img_input],\n",
        "                                            samples=[[path.as_posix()]\n",
        "                                                     for path in sorted(pathlib.Path('images').rglob('*.JPG'))])\n",
        "\n",
        "            img_but = gr.Button('Detectar')\n",
        "        with gr.TabItem('Webcam'):\n",
        "            with gr.Row():\n",
        "                webcam_input = gr.Image(type='pil', label='Capturar la imagen desde la webcam')\n",
        "                webcam_output = gr.Image()\n",
        "\n",
        "            webcam_but = gr.Button('Detectar desde la Webcam')\n",
        "\n",
        "    # Tus botones existentes\n",
        "\n",
        "    # Botón para detectar desde URL\n",
        "    url_but.click(\n",
        "        detect_objects,\n",
        "        inputs=[options, url_input, gr.Image(value=None), slider_input],\n",
        "        outputs=img_output_from_url,\n",
        "        queue=True\n",
        "    )\n",
        "\n",
        "    # Botón para detectar desde carga de imagen\n",
        "    img_but.click(\n",
        "        detect_objects,\n",
        "        inputs=[options, gr.Textbox(value=\"\"), img_input, slider_input],\n",
        "        outputs=img_output_from_upload,\n",
        "        queue=True\n",
        "    )\n",
        "\n",
        "    # Botón para detectar desde la webcam\n",
        "    webcam_but.click(\n",
        "        detect_objects,\n",
        "        inputs=[options, gr.Textbox(value=\"\"), webcam_input, slider_input],\n",
        "        outputs=webcam_output,\n",
        "        queue=True\n",
        "    )\n",
        "    example_images.click(fn=set_example_image,inputs=[example_images],outputs=[img_input])\n",
        "    example_url.click(fn=set_example_url,inputs=[example_url],outputs=[url_input])\n",
        "\n",
        "app = demo.launch(share=True)\n",
        "\n",
        "# La URL pública se imprimirá automáticamente en la salida de Colab\n",
        "# Pero también puedes obtenerla y guardarla en una variable así\n",
        "#public_url = app.share_url\n",
        "\n",
        "# Imprimir la URL pública\n",
        "#print(\"Public URL:\", public_url)"
      ],
      "metadata": {
        "id": "y_CiV5obzkAw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "e33324d5-a1e5-49ef-8fe7-55e57b12db84"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://564d79b3ee0867dbfc.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://564d79b3ee0867dbfc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import qrcode\n",
        "\n",
        "# Create a QR code object with a larger size and higher error correction\n",
        "qr = qrcode.QRCode(version=3, box_size=20, border=10, error_correction=qrcode.constants.ERROR_CORRECT_H)\n",
        "\n",
        "# Define the data to be encoded in the QR code\n",
        "data = \"https://e62cd67e-7e77-491f.gradio.live/\"\n",
        "\n",
        "# Add the data to the QR code object\n",
        "qr.add_data(data)\n",
        "\n",
        "# Make the QR code\n",
        "qr.make(fit=True)\n",
        "\n",
        "# Create an image from the QR code with a black fill color and white background\n",
        "img = qr.make_image(fill_color=\"black\", back_color=\"white\")\n",
        "\n",
        "# Save the QR code image\n",
        "img.save(\"qr_code.png\")"
      ],
      "metadata": {
        "id": "tZJjX-MWfrFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "M-jHEGQ2TRfK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}